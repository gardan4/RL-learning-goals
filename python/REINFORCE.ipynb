{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPvSHc5dpZRNg6s9LUrDuep"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install gymnasium"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LInO_osRdsrR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716035867152,
     "user_tz": -120,
     "elapsed": 12992,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    },
    "outputId": "201f63e4-c61f-4293-9697-48f92aa2a0df",
    "ExecuteTime": {
     "end_time": "2024-05-18T12:44:04.371616Z",
     "start_time": "2024-05-18T12:43:56.724927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Using cached gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\marc\\anaconda3\\envs\\xai-project\\lib\\site-packages (from gymnasium) (1.24.3)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium)\n",
      "  Using cached cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\marc\\anaconda3\\envs\\xai-project\\lib\\site-packages (from gymnasium) (4.10.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "Using cached cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, cloudpickle, gymnasium\n",
      "Successfully installed cloudpickle-3.0.0 farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping C:\\Users\\Marc\\anaconda3\\envs\\xai-project\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\Marc\\anaconda3\\envs\\xai-project\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\Marc\\anaconda3\\envs\\xai-project\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\Marc\\anaconda3\\envs\\xai-project\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\Marc\\anaconda3\\envs\\xai-project\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n",
      "WARNING: Skipping C:\\Users\\Marc\\anaconda3\\envs\\xai-project\\Lib\\site-packages\\numpy-1.26.4.dist-info due to invalid metadata entry 'name'\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ksz5WUx9nYsY",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716035868405,
     "user_tz": -120,
     "elapsed": 1256,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    },
    "outputId": "436b01b6-22d1-42df-f203-283f0445052a"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "I35GLI-sdNxv",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716035871776,
     "user_tz": -120,
     "elapsed": 3373,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c59e1e36-800b-4ba8-cba2-bc759789d1d9",
    "ExecuteTime": {
     "end_time": "2024-05-18T12:44:09.975807Z",
     "start_time": "2024-05-18T12:44:09.207917Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Create the MountainCarContinuous environment\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# Set parameters\n",
    "num_episodes = 100\n",
    "learning_rate = 0.01  # Lower learning rate for stability in continuous action space\n",
    "gamma = 0.99\n",
    "\n",
    "# Define the policy network for continuous action space using Gaussian distribution\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        # hidden layers\n",
    "        self.fc1 = nn.Linear(input_dim, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        # Give the mean and std a seperate hidden layer\n",
    "        self.fc3_mean = nn.Linear(24, 12)\n",
    "        self.fc3_log_std = nn.Linear(24, 12)\n",
    "        # output\n",
    "        self.fc_mean = nn.Linear(12, output_dim)\n",
    "        self.fc_log_std = nn.Linear(12, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mean = self.fc3_mean(x)\n",
    "        mean = self.fc_mean(mean)\n",
    "        log_std = self.fc3_log_std(x)\n",
    "        log_std = self.fc_log_std(log_std)\n",
    "        std = torch.exp(log_std)\n",
    "        return mean, std\n",
    "\n",
    "\n",
    "# Initialize policy network and optimizer\n",
    "policy_net = PolicyNetwork(env.observation_space.shape[0], env.action_space.shape[0]).to(device)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgymnasium\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01moptim\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'torch'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "policy_net.load_state_dict(torch.load(\"./model1.pth\"))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CrY84LRVn8MX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1716035871776,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    },
    "outputId": "37e34dcb-e951-4546-bffa-e99fdca774a9"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to choose action based on policy\n",
    "def choose_action(state):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    mean, std = policy_net(state)\n",
    "    normal = torch.distributions.Normal(mean, std)\n",
    "    action = normal.sample()\n",
    "    return action.detach().cpu().numpy().flatten()\n",
    "\n",
    "# Function to compute the discounted rewards\n",
    "def discount_rewards(rewards, gamma):\n",
    "    discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "    cumulative = 0.0\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        cumulative = cumulative * gamma + rewards[t]\n",
    "        discounted_rewards[t] = cumulative\n",
    "    return discounted_rewards\n",
    "\n",
    "# REINFORCE Algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()[0]\n",
    "    states, actions, rewards, log_probs = [], [], [], []\n",
    "\n",
    "    # Generate an episode\n",
    "    while True:\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        # Compute the log probability of the action\n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        mean, std = policy_net(state_tensor)\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        log_prob = normal.log_prob(torch.tensor(action, dtype=torch.float32).to(device)).sum()\n",
    "        log_probs.append(log_prob)\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Convert lists to arrays\n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    log_probs = torch.stack(log_probs)\n",
    "\n",
    "    # Compute discounted rewards\n",
    "    discounted_rewards = discount_rewards(rewards, gamma)\n",
    "\n",
    "    # Normalize discounted rewards\n",
    "    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32).to(device)\n",
    "    discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-7)\n",
    "\n",
    "    # Update policy network\n",
    "    optimizer.zero_grad()\n",
    "    loss = -torch.sum(log_probs * discounted_rewards)  # Negative for gradient ascent\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print episode information\n",
    "    print(f'Episode: {episode+1}, Total Reward: {np.sum(rewards)}')\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "NuiidCD4g9XT",
    "executionInfo": {
     "status": "error",
     "timestamp": 1716036156830,
     "user_tz": -120,
     "elapsed": 285057,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    },
    "outputId": "33166cd8-151d-4614-c03a-fdf6509a5cac"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Episode: 1, Total Reward: -113.91503434582015\n",
      "Episode: 2, Total Reward: 26.560122814670997\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-d0cf28f24dbb>\u001B[0m in \u001B[0;36m<cell line: 19>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;31m# Generate an episode\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mchoose_action\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     26\u001B[0m         \u001B[0mnext_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreward\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m         \u001B[0mstates\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-5-d0cf28f24dbb>\u001B[0m in \u001B[0;36mchoose_action\u001B[0;34m(state)\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mmean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpolicy_net\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mnormal\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdistributions\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mNormal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmean\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m     \u001B[0maction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnormal\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0maction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcpu\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/normal.py\u001B[0m in \u001B[0;36msample\u001B[0;34m(self, sample_shape)\u001B[0m\n\u001B[1;32m     66\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     67\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample_shape\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 68\u001B[0;31m         \u001B[0mshape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_extended_shape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     69\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnormal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscale\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\u001B[0m in \u001B[0;36m_extended_shape\u001B[0;34m(self, sample_shape)\u001B[0m\n\u001B[1;32m    266\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_shape\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    267\u001B[0m             \u001B[0msample_shape\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 268\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msample_shape\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_batch_shape\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_event_shape\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    269\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    270\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_validate_sample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(policy_net.state_dict(), \"/content/drive/Othercomputers/My Computer/python/model2.pth\")"
   ],
   "metadata": {
    "id": "83EJdtmonLXc",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1716036156830,
     "user_tz": -120,
     "elapsed": 2,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to play the game with the learned policy and render it\n",
    "def play_game(policy_net, num_episodes=5):\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            action = choose_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # Render the environment\n",
    "            img = env.render(mode='human')\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            display(plt.gcf())\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        print(f'Episode {episode+1}: Total Reward: {total_reward}')\n",
    "    env.close()\n",
    "\n",
    "# Play the game with the learned policy\n",
    "play_game(policy_net)\n"
   ],
   "metadata": {
    "id": "RPT4sfY5ga7e",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1716036156831,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Function to visualize the policy\n",
    "def visualize_policy(policy_net):\n",
    "    x = np.linspace(env.observation_space.low[0], env.observation_space.high[0], 100)\n",
    "    v = np.linspace(env.observation_space.low[1], env.observation_space.high[1], 100)\n",
    "    X, V = np.meshgrid(x, v)\n",
    "    actions = np.zeros_like(X, dtype=np.float32)\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            state = np.array([X[i, j], V[i, j]])\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            mean, std = policy_net(state_tensor)\n",
    "            action = mean.detach().cpu().numpy().flatten()[0]  # Use mean action for visualization\n",
    "            actions[i, j] = action\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(X, V, actions, levels=100, cmap='coolwarm')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Position (x)')\n",
    "    plt.ylabel('Velocity (v)')\n",
    "    plt.title('Learned Policy Visualization')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the policy\n",
    "visualize_policy(policy_net)\n"
   ],
   "metadata": {
    "id": "4lGHU339hf6c",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1716036156831,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "marc meijers",
      "userId": "04525763475681657889"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
