{"cells":[{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9Izdr8jcbz9","executionInfo":{"status":"ok","timestamp":1716031470624,"user_tz":-120,"elapsed":21931,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}},"outputId":"907e72ae-9dd1-41ac-d445-da3afde8c746"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"6T7zAqlJcIUi","executionInfo":{"status":"ok","timestamp":1716032082896,"user_tz":-120,"elapsed":216,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}}},"outputs":[],"source":["import copy\n","import numpy as np\n","import gym\n","\n","class VectorizedEnvWrapper(gym.Wrapper):\n","    def __init__(self, env, num_envs=1):\n","        '''\n","        env (gym.Env): to make copies of\n","        num_envs (int): number of copies\n","        '''\n","        super().__init__(env)\n","        self.num_envs = num_envs\n","        self.envs = [copy.deepcopy(env) for n in range(num_envs)]\n","\n","    def reset(self):\n","        '''\n","        Return and reset each environment\n","        '''\n","        return np.asarray([env.reset() for env in self.envs])\n","\n","    def step(self, actions):\n","        '''\n","        Take a step in the environment and return the result.\n","        actions (torch.tensor)\n","        '''\n","        next_states, rewards, dones = [], [], []\n","        for env, action in zip(self.envs, actions):\n","            next_state, reward, done, _ = env.step(action.item())\n","            if done:\n","                next_states.append(env.reset())\n","            else:\n","                next_states.append(next_state)\n","            rewards.append(reward)\n","            dones.append(done)\n","        return np.asarray(next_states), np.asarray(rewards), \\\n","            np.asarray(dones)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VmFBDdG5cIUj","executionInfo":{"status":"ok","timestamp":1716031475879,"user_tz":-120,"elapsed":4485,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}},"outputId":"49d3dcad-4602-4686-c8bb-ada80d0a6e9c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["import torch\n","\n","class Policy:\n","    def pi(self, s_t):\n","        '''\n","        returns the probability distribution over actions\n","        (torch.distributions.Distribution)\n","\n","        s_t (np.ndarray): the current state\n","        '''\n","        raise NotImplementedError\n","\n","    def act(self, s_t):\n","        '''\n","        s_t (np.ndarray): the current state\n","        Because of environment vectorization, this will produce\n","        E actions where E is the number of parallel environments.\n","        '''\n","        a_t = self.pi(s_t).sample()\n","        return a_t\n","\n","    def learn(self, states, actions, returns):\n","        '''\n","        states (np.ndarray): the list of states encountered during\n","                             rollout\n","        actions (np.ndarray): the list of actions encountered during\n","                              rollout\n","        returns (np.ndarray): the list of returns encountered during\n","                              rollout\n","\n","        Because of environment vectorization, each of these has first\n","        two dimensions TxE where T is the number of time steps in the\n","        rollout and E is the number of parallel environments.\n","        '''\n","        actions = torch.tensor(actions)\n","        returns = torch.tensor(returns)\n","\n","        log_prob = self.pi(states).log_prob(actions)\n","        loss = torch.mean(-log_prob*returns)\n","        self.opt.zero_grad()\n","        loss.backward()\n","        self.opt.step()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJRBJbP9cIUk","executionInfo":{"status":"ok","timestamp":1716031475880,"user_tz":-120,"elapsed":5,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}},"outputId":"a18ac38f-e1e3-48ea-81b0-3c6b881a1d52"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["class DiagonalGaussianPolicy(Policy):\n","    def __init__(self, env, lr=1e-2):\n","        '''\n","        env (gym.Env): the environment\n","        lr (float): learning rate\n","        '''\n","        self.N = env.observation_space.shape[0]\n","        self.M = env.action_space.shape[0]\n","\n","        self.mu = torch.nn.Sequential(\n","            torch.nn.Linear(self.N, 64),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(64, 64),\n","            torch.nn.ReLU(),\n","            torch.nn.Linear(64, self.M)\n","        ).double()\n","\n","        self.log_sigma = torch.ones(self.M, dtype=torch.double, requires_grad=True)\n","\n","        self.opt = torch.optim.Adam(list(self.mu.parameters()) + [self.log_sigma], lr=lr)\n","\n","    def pi(self, s_t):\n","        '''\n","        returns the probability distribution over actions\n","        s_t (np.ndarray): the current state\n","        '''\n","        s_t = torch.as_tensor(s_t).double()\n","        mu = self.mu(s_t)\n","        log_sigma = self.log_sigma\n","        sigma = torch.exp(log_sigma)\n","        pi = torch.distributions.MultivariateNormal(mu, torch.diag(sigma))\n","        return pi"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"z53nqSipcIUk","executionInfo":{"status":"ok","timestamp":1716031475880,"user_tz":-120,"elapsed":4,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}}},"outputs":[],"source":["class CategoricalPolicy(Policy):\n","    def __init__(self, env, lr=1e-2):\n","        '''\n","        env (gym.Env): the environment\n","        lr (float): learning rate\n","        '''\n","        self.N = env.observation_space.shape[0]\n","        self.M = env.action_space.n\n","        self.p = torch.nn.Sequential(\n","            torch.nn.Linear(self.N, self.M),\n","        ).double()\n","\n","        self.opt = torch.optim.Adam(self.p.parameters(), lr=lr)\n","\n","    def pi(self, s_t):\n","        '''\n","        returns the probability distribution over actions\n","        s_t (np.ndarray): the current state\n","        '''\n","        s_t = torch.as_tensor(s_t).double()\n","        p = self.p(s_t)\n","        pi = torch.distributions.Categorical(logits=p)\n","        return pi"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"sywtyE47cIUk","executionInfo":{"status":"ok","timestamp":1716031475880,"user_tz":-120,"elapsed":4,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}}},"outputs":[],"source":["def calculate_returns(rewards, dones, gamma):\n","    result = np.empty_like(rewards)\n","    result[-1] = rewards[-1]\n","    for t in range(len(rewards)-2, -1, -1):\n","        result[t] = rewards[t] + gamma*(1-dones[t])*result[t+1]\n","    return result"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"RoKH6a3IcIUl","executionInfo":{"status":"ok","timestamp":1716031478433,"user_tz":-120,"elapsed":2557,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}}},"outputs":[],"source":["import seaborn as sns; sns.set()\n","\n","def REINFORCE(env, agent, gamma=0.99, epochs=100, T=1000):\n","    # for learning\n","    states = np.empty((T, env.num_envs, agent.N))\n","    if isinstance(env.action_space, gym.spaces.Discrete):\n","        # discrete action spaces only need to store a\n","        # scalar for each action.\n","        actions = np.empty((T, env.num_envs))\n","    else:\n","        # continuous action spaces need to store a\n","        # vector for each eaction.\n","        actions = np.empty((T, env.num_envs, agent.M))\n","    rewards = np.empty((T, env.num_envs))\n","    dones = np.empty((T, env.num_envs))\n","\n","    # for plotting\n","    totals = []\n","\n","    for epoch in range(epochs):\n","        s_t = env.reset()\n","\n","        for t in range(T):\n","            a_t = agent.act(s_t)\n","            s_t_next, r_t, d_t = env.step(a_t)\n","\n","            # for learning\n","            states[t] = s_t\n","            actions[t] = a_t\n","            rewards[t] = r_t\n","            dones[t] = d_t\n","\n","            s_t = s_t_next\n","\n","        returns = calculate_returns(rewards, dones, gamma)\n","        agent.learn(states, actions, returns)\n","\n","        # for plotting\n","        # average reward = total reward/number of episodes\n","        totals.append(rewards.sum()/dones.sum())\n","        print(f'{epoch}/{epochs}:{totals[-1]}\\r', end='')\n","\n","    sns.lineplot(x=range(len(totals)), y=totals)\n","\n","    return agent"]},{"cell_type":"markdown","metadata":{"id":"wcBy4DkDcIUl"},"source":["First we will run a `Categorical` policy on the `CartPole-v0` environment from the previous post. This environment is very simple and works well as a first test bed for algorithms using discrete action spaces."]},{"cell_type":"markdown","metadata":{"id":"GJH5K2fScIUl"},"source":["Because the environment has a small time horizon ($T=200$) we will use $\\gamma=0.99$ which is the default. Also, we will use 32 environment, which is a good balance between wall clock time and improved data collection. Because we are using a basic linear policy, we can use a relatively high learning rate of 0.1."]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"d3KKjDcGcIUl","executionInfo":{"status":"error","timestamp":1716032100491,"user_tz":-120,"elapsed":231,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}},"outputId":"3bb85169-8eea-4dfc-e213-2bc2b0d5e0a7"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'Box' object has no attribute 'n'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-c7d30df84fef>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectorizedEnvWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MountainCarContinuous-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_envs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREINFORCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-f69ebe44732f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, lr)\u001b[0m\n\u001b[1;32m      6\u001b[0m         '''\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         self.p = torch.nn.Sequential(\n\u001b[1;32m     10\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"]}],"source":["env = VectorizedEnvWrapper(gym.make(\"MountainCarContinuous-v0\"), num_envs=32)\n","agent = CategoricalPolicy(env, lr=1e-1)\n","agent = REINFORCE(env, agent)"]},{"cell_type":"markdown","metadata":{"id":"ozWZYcXPcIUm"},"source":["For the next environment, we will be using `CartPoleSwingUp-v0`, which is *not* included in the basic `gym` installation. It can be installed with `pip install gym-cartpole-swingup`, and then imported to register the environment with `gym` (so we can create it via `gym.make`).\n","\n","This environment is very simple. It is like `CartPole-v0`, except the pole begins hanging down and must be swung into the upward position. The agent must learn to rapidly accelerate to add momentum to the pole, then switch directions to bring the pole above horizontal. This requires some level of exploration, so to improve our odds of randomly choosing good actions we massively scale up the number of parallel environments to 256. We also increase $\\gamma = 0.999$ to account for the longer time horizon of this problem (capped at the length of the rollout, $T=1000$). With this many environments, we only need about 20 epochs to get a decent policy. We decrease the learning rate since we are using a larger network (2 hidden layers) and since the problem is harder and we don't want to make updates that overshoot the optimal policy.\n","\n","In honesty, running this code with more environments, longer rollouts, more epochs and a smaller learning rate might be able to discover a really good policy that consistently performs well."]},{"cell_type":"code","execution_count":18,"metadata":{"id":"ZTyJi12hcIUm","executionInfo":{"status":"ok","timestamp":1716032105128,"user_tz":-120,"elapsed":220,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}}},"outputs":[],"source":["import gymnasium as gym\n","env = VectorizedEnvWrapper(gym.make(\"MountainCarContinuous-v0\"), num_envs=256)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":316},"id":"mUxX_qkDcIUm","executionInfo":{"status":"error","timestamp":1716032106146,"user_tz":-120,"elapsed":3,"user":{"displayName":"marc meijers","userId":"04525763475681657889"}},"outputId":"9d777fc5-2f21-42ae-eee3-94f14bafa1a1"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-7f4e6c58d96a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiagonalGaussianPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREINFORCE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-1450c5ade27e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, lr)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_sigma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mmaximize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapturable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcapturable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         differentiable=differentiable, fused=fused)\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mallowed_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcode_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcode_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvert_frame\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moutput_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOutputGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mreplay_record\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExecutionRecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInstructionTranslator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpeculationLog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweak\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWeakTensorKeyDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchdynamo_logging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompiledFn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompilerFn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m from .bytecode_transformation import (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mUnspecializedPythonVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m )\n\u001b[0;32m---> 68\u001b[0;31m from .torch import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mTorchCtxManagerClassVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mTorchInGraphFunctionVariable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_dynamo/variables/torch.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_symbolic_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fx_tracing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_onnx_export\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compiling\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m }\n","\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'torch._dynamo' has no attribute 'external_utils' (most likely due to a circular import)"]}],"source":["agent = DiagonalGaussianPolicy(env, lr=1e-2)\n","agent = REINFORCE(env, agent, gamma=0.999, epochs=20)"]},{"cell_type":"markdown","metadata":{"id":"iNL7VKVLcIUm"},"source":["I hope that this thorough introduction to policy gradients has been helpful. In the next posts, we will diver deeper into actor-critic methods and more advanced policy gradient methods."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}