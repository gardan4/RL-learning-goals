{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KEN4157 - Reinforcement Learning - Cliff Walking Gymnasium\n",
        "If you opened this notebook in Google Colab, we recommend to start by saving a copy of the notebook in your own Google Drive, such that you can save any of your changes and experiments."
      ],
      "metadata": {
        "id": "TZQ5cRaoNmYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing & Importing Modules\n",
        "We will start by installing importing some modules that will likely be useful for your assignment(s). This includes [Gymnasium](https://gymnasium.farama.org/), which is a framework containing many popular RL environments (a successor to the original Gym API from OpenAI)."
      ],
      "metadata": {
        "id": "tiobt1sTN2hG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium\n",
        "\n",
        "import gymnasium as gym\n",
        "import itertools\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.tri import Triangulation\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Make plots look nice\n",
        "sns.set()\n",
        "sns.set_context(\"notebook\")\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "metadata": {
        "id": "Jd_DIXIdN5qT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ddfac79-1109-44c1-e924-e8519fe17e8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the Cliff Walking Environment\n",
        "Here, we'll set up the Cliff Walking environment, and have a look at how to interact with it according to the Gym API.\n",
        "\n",
        "**Optional**:\n",
        "- For a description and documentation of the environment, see: https://gymnasium.farama.org/environments/toy_text/cliff_walking/\n",
        "- For the implementation of the environment, see: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/toy_text/cliffwalking.py"
      ],
      "metadata": {
        "id": "9J790RSkOXIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CliffWalking-v0')\n",
        "\n",
        "action_space = env.action_space\n",
        "obs_space = env.observation_space\n",
        "\n",
        "# We'll define a few strings here, because proper names are sometimes nicer\n",
        "# to work with (print) than arbitrary indices\n",
        "ACTION_NAMES = [\"Move Up\", \"Move Right\", \"Move Down\", \"Move Left\"]\n",
        "\n",
        "print(f\"There are {action_space.n} different actions in this environment.\")\n",
        "for i in range(action_space.n):\n",
        "  print(f\"Action {i} = {ACTION_NAMES[i]}\")\n",
        "print(f\"There are {obs_space.n} discrete states in this environment.\")"
      ],
      "metadata": {
        "id": "XKhcxGIpnEkB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01030ede-fdda-42cf-c3fe-fa09222ebef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 4 different actions in this environment.\n",
            "Action 0 = Move Up\n",
            "Action 1 = Move Right\n",
            "Action 2 = Move Down\n",
            "Action 3 = Move Left\n",
            "There are 48 discrete states in this environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualisation Code\n",
        "Some functions here that let us nicely visualise Q-value tables for the Cliff Walking environment. No need to touch any of this, but you should run the code block, such that you can use the functions later on."
      ],
      "metadata": {
        "id": "_t_13siu9rNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Thanks to: https://stackoverflow.com/a/66050636/6735980\n",
        "def triangulation_for_triheatmap(num_cols, num_rows):\n",
        "    xv, yv = np.meshgrid(np.arange(-0.5, num_cols), np.arange(-0.5, num_rows))  # vertices of the little squares\n",
        "    xc, yc = np.meshgrid(np.arange(0, num_cols), np.arange(0, num_rows))  # centers of the little squares\n",
        "    x = np.concatenate([xv.ravel(), xc.ravel()])\n",
        "    y = np.concatenate([yv.ravel(), yc.ravel()])\n",
        "    cstart = (num_cols + 1) * (num_rows + 1)  # indices of the centers\n",
        "\n",
        "    trianglesN = [(i + j * (num_cols + 1), i + 1 + j * (num_cols + 1), cstart + i + j * num_cols)\n",
        "                  for j in range(num_rows) for i in range(num_cols)]\n",
        "    trianglesE = [(i + 1 + j * (num_cols + 1), i + 1 + (j + 1) * (num_cols + 1), cstart + i + j * num_cols)\n",
        "                  for j in range(num_rows) for i in range(num_cols)]\n",
        "    trianglesS = [(i + 1 + (j + 1) * (num_cols + 1), i + (j + 1) * (num_cols + 1), cstart + i + j * num_cols)\n",
        "                  for j in range(num_rows) for i in range(num_cols)]\n",
        "    trianglesW = [(i + (j + 1) * (num_cols + 1), i + j * (num_cols + 1), cstart + i + j * num_cols)\n",
        "                  for j in range(num_rows) for i in range(num_cols)]\n",
        "    return [Triangulation(x, y, triangles) for triangles in [trianglesN, trianglesE, trianglesS, trianglesW]]\n",
        "\n",
        "# Thanks to: https://stackoverflow.com/a/66050636/6735980\n",
        "def create_qvals_heatmap(q_vals_up, q_vals_right, q_vals_down, q_vals_left):\n",
        "    NUM_COLS = 12\n",
        "    NUM_ROWS = 4\n",
        "\n",
        "    values = [q_vals_up, q_vals_right, q_vals_down, q_vals_left]\n",
        "    triangul = triangulation_for_triheatmap(NUM_COLS, NUM_ROWS)\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    imgs = [ax.tripcolor(t, val.ravel(), cmap='RdYlGn',\n",
        "                        norm=colors.SymLogNorm(linthresh=20.0, linscale=1.0,\n",
        "                                               vmin=np.min(values),\n",
        "                                               vmax=np.max(values), base=10)\n",
        "                        )\n",
        "            for t, val in zip(triangul, values)]\n",
        "    cbar = fig.colorbar(imgs[0], ax=ax)\n",
        "\n",
        "    ax.set_xticks(range(NUM_COLS))\n",
        "    ax.set_yticks(range(NUM_ROWS))\n",
        "    ax.invert_yaxis()\n",
        "    ax.margins(x=0, y=0)\n",
        "    ax.set_aspect('equal', 'box')  # square cells\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_returns_per_episode(returns_per_episode):\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot([episode for episode in range(len(returns_per_episode))], returns_per_episode, label=\"Returns per Episode\")\n",
        "    ax.set_xlabel(\"Episode\")\n",
        "    ax.set_ylabel(f\"Returns\")\n",
        "    ax.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "lHdos1Eo92jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Value Function & Policy\n",
        "In the below code block, we'll:\n",
        "- fill up a Q(s, a) table with completely random numbers\n",
        "- run through a bunch of steps with a random policy\n",
        "- demonstrate how the above functions can be used to visualise this random Q-table and policy\n",
        "\n",
        "Afterwards, you can implement your own (non-random) algorithms and use the plotting functions in the same way."
      ],
      "metadata": {
        "id": "mP1Q1hoSBSNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_q_table = np.random.random((obs_space.n, action_space.n))\n",
        "returns_per_episode = []\n",
        "\n",
        "observation, info = env.reset()\n",
        "current_episode_returns = 0.0\n",
        "\n",
        "for step in tqdm(range(100_000)):\n",
        "    action = action_space.sample()\n",
        "\n",
        "    new_obs, reward, terminated, truncated, info = env.step(action)\n",
        "    current_episode_returns = current_episode_returns + reward\n",
        "\n",
        "    if terminated or truncated:\n",
        "        # episode ended: store data and reset to new episode\n",
        "        returns_per_episode.append(current_episode_returns)\n",
        "        current_episode_returns = 0.0\n",
        "        observation, info = env.reset()\n",
        "    else:\n",
        "        observation = new_obs\n",
        "\n",
        "# Plot returns we got for each completed episode\n",
        "plot_returns_per_episode(returns_per_episode)\n",
        "\n",
        "# Plot heatmap from our Q-table (which in this case is just random numbers)\n",
        "q_vals_up = random_q_table[:, 0]\n",
        "q_vals_right = random_q_table[:, 1]\n",
        "q_vals_down = random_q_table[:, 2]\n",
        "q_vals_left = random_q_table[:, 3]\n",
        "create_qvals_heatmap(q_vals_up, q_vals_right, q_vals_down, q_vals_left)"
      ],
      "metadata": {
        "id": "nbCb42nDHIRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Code\n",
        "Your own code should probably go below here!"
      ],
      "metadata": {
        "id": "VNe_GhVmJJkm"
      }
    }
  ]
}